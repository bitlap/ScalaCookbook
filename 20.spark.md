# 20. Apache Spark

本章展示了Apache Spark（ *https://spark.apache.org* ）的一些例子，这是一个用于大规模数据处理的统一数据分析引擎。

Spark网站将其描述为一个“用于大规模数据处理的统一分析引擎”。这意味着它是一个大数据框架，可以让你用不同的技术分析你的数据 —— 比如把数据当作电子表格或数据库 —— 并在分布式集群上运行。你可以用Spark来分析那些大到跨越数千台计算机的数据集。
虽然Spark是为在计算机集群上处理巨大的数据集而设计的，但它的一个伟大之处在于，你只需用几个例子文件就可以学会如何在自己的计算机上使用Spark。

#### Spark 3.1.1 -- TODO 鸽子栏

&nbsp;&nbsp;&nbsp;&nbsp;本章的例子使用Spark 3.1.1，该版本于2021年3月发布，也是本文写作时的最新版本。Spark目前只能与Scala 2.12一起工作，所以本章的例子也使用Scala 2.12。然而，由于使用Spark的工作通常涉及到使用 **map** 和 **filter** 等集合方法，或者SQL查询，所以在这些例子中你几乎不会注意到Scala 2和Scala 3之间的区别。

本章的例子展示了如何在你自己的计算机上使用Spark，同时展示了在跨越数千台计算机的数据集上工作的关键概念。20.1小节展示了如何开始使用Spark，并深入研究其基本概念之一，即 *弹性分布式数据集（Resilient Distributed Dataset）*，或称RDD。RDD让你把数据当作一个大型的分布式电子表格。

第一个例子展示了如何在REPL内部，从一个集合中创建一个RDD，然后20.2小节展示了如何将一个数据文件读入RDD。它展示了几种读取数据文件的方法，使用Scala和Spark方法操作该数据，并将数据写入其他文件中。

接下来，使用MovieLens（ *https://movielens.org* ）的样本数据集，20.3小节展示了如何用Scala样例类对CSV文件中的一行进行建模，然后将该文件中的数据读入RDD并按需要进行操作。

然后，20.4小节展示了如何将RDD转换成 **DataFrame**。这个过程让你可以使用类似SQL的API来查询你的数据。这些例子展示了如何使用该查询语法，以及如何定义一个模式来表示你的数据。

从20.5小节开始，展示了如何将MovieLens CSV文件读入一个 **DataFrame**，并再次查询该数据。本示例将这一过程向前推进了一步，展示了如何在该数据上创建一个SQL视图，这让你可以使用常规的SQL查询字符串来查询该数据。

随着例子继续使用MovieLens数据集，20.6小节展示了如何将多个CSV文件读入 **DataFrames**，在这些文件上创建视图，然后用传统的SQL查询（包括Join）来查询数据，比如这样：
```scala
    val query = """
        select * from movies, ratings
        where movies.movieId == ratings.movieId
        and rating == 5.0
    """
```

最后，20.7小节展示了创建Spark应用程序的完整过程，该程序可以使用sbt打包成JAR文件并从命令行运行。

Spark是一个巨大的主题，关于它的书已经写了很多。本章内容的目的是让你迅速掌握一些主要的技术，并使之运行。有关Spark的更多细节，请看Bill Chambers和Matei Zaharia（O'Reilly）写的 *Spark: The Definitive Guide* 一书。

#### MovieLens数据集 -- TODO 鸽子栏

&nbsp;本章的例子使用MovieLens数据集（ *https://oreil.ly/3fSf0* ）。该数据集包括以CSV文件形式存在的真实世界的电影评分，该文件大小为数百兆字节，包含了超过62,000部电影的评分和细节。关于该数据集的细节，请参阅F.Maxwell Harper和Joseph A. Konstan的开创性论文，“The MovieLens Datasets: History and Context,” ACM Transactions on Interactive Intelligent Systems 5, no. 4 (2016): 1– 19, *https://doi.org/10.1145/2827872* .

## 20.1 开始使用Spark

### 问题

你以前从未使用过Spark，并想开始使用它。

### 解决方案

当你专业地使用Spark时，你的数据可能会分布在多台电脑上 —— 也许是成千上万台电脑 —— 但要开始使用Spark，你可以在一台电脑系统上以本地模式完成所有的工作。最简单的方法是启动Spark shell，创建一个数组，然后从那里开始。

Spark的安装说明可能会随着时间的推移而变化，但目前你只需从其下载页面（ *https://oreil.ly/O63Qu* ）下载Spark。下载的文件是一个 *tgz* 文件，所以只要解压并将其移动到你的 *bin* 目录，就像你手动安装其他下载的应用程序一样。例如，我把Spark安装在我的 */Users/al/bin* 目录下。

一旦你安装了Spark，就可以像这样启动Scala Spark shell：
```
    $ spark-shell
```

Spark shell是你通过 **scala** 命令得到的普通Scala shell的修改版，所以在Scala shell中能做的任何事情，在Spark shell中也能做，比如创建一个数组：
```scala
    val nums = Array.range(0, 100)    
```

一旦你有了像数组（array）或映射（map）这样的东西，就可以通过调用Spark Context的 **parallelize** 方法创建一个Spark Resilient Distributed Dataset（RDD）：
```scala
    scala> val rdd = spark.sparkContext.parallelize(nums)
    rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize
             at <console>:25
```

从输出中注意到，rdd的类型为 **RDD[Int]** 。正如你将在讨论中看到的，RDD是Spark的基本构建模块之一。现在你可以把它看作是一个像列表或数组一样的集合类，但它的数据可以分布在集群中的所有计算机上。它也有额外的方法可以被调用。下面是一些看起来很熟悉的Scala集合类的方法的例子：
```scala
    rdd.count
    rdd.first
    rdd.min
    rdd.max
    rdd.take(3)
    rdd.take(2).foreach(println)
    // Long = 100
    // Int = 0
    // Int = 0
    // Int = 99
    // Array[Int] = Array(0, 1, 2)
    // prints 0 and 1 on separate lines
```

这里有几个RDD方法，可能看起来不那么熟悉：
```scala
    // “sample” methods return random values from the RDD
    rdd.sample(false, 0.05).collect   // Array[Int] = Array(0, 16, 22, 27, 60, 73)
    rdd.takeSample(false, 5)          // Array[Int] = Array(35, 65, 31, 27, 1)
    
    rdd.mean                          // Double = 49.5
    rdd.stdev                         // Double = 28.866070047722115
    rdd.getNumPartitions              // Int = 8
    rdd.stats                         // StatCounter = (count: 100, mean: 49.500000,
                                      // stdev: 28.866070, max: 99.000000,
                                      // min: 0.000000)
```

你也可以使用熟悉的集合方法，如 **map** 和 **filter**：
```scala
    scala> rdd.map(_ + 5).filter(_ < 8)
    res0: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at filter at 
          <console>:26
    
    scala> rdd.filter(_ > 10).filter(_ < 20)
    res1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[12] at filter at
          <console>:26
```

然而，注意到这些方法并没有返回一个结果，至少不是你所期望的那个结果。在Spark中，像这样的转换方法是被懒惰地评估的，所以我们把它们称为懒惰或非严格的。为了从它们那里得到一个结果，你必须添加一个action方法。一个RDD有一个 **collect** 方法，这是一个action方法，可以强制运行所有之前的转换方法，然后把结果带回你的代码正在运行的计算机上。在这些例子中，添加 **collect** 会导致一个结果被计算出来：
```scala
    scala> rdd.map(_ + 5).filter(_ < 8).collect 
    res0: Array[Int] = Array(5, 6, 7)

    scala> rdd.filter(_ > 10).filter(_ < 20).collect
    res1: Array[Int] = Array(11, 12, 13, 14, 15, 16, 17, 18, 19)
```

### 讨论

在生产环境下，Spark会处理分布在计算机集群中的数据，但如图所示，在Spark的本地模式下，所有的处理都在你的本地计算机上完成。

#### spark对象和spark上下文

在解决方案中，我在Spark shell中用这两行代码创建了一个RDD：
```scala
    val nums = Array.range(0, 100)
    val rdd = spark.sparkContext.parallelize(nums)
```

正如你可能猜到的，spark是一个在shell中可用的预先构建的对象。你可以用shell的 **:type** 命令查看 **spark** 和 **spark.sparkContext** 的类型：
```scala
    scala> :type spark
    org.apache.spark.sql.SparkSession
    
    scala> :type spark.sparkContext
    org.apache.spark.SparkContext
```

在你的Spark编程中，你会经常使用这两个对象。因为你经常使用 **SparkContext**，所以在shell中有一个名为 **sc** 的快捷键可用，而不要再输入这个：
```scala
    val rdd = spark.sparkContext.parallelize(nums)
```

你可以直接打这个：
```scala
    val rdd = sc.parallelize(nums)
```

#### RDD

虽然在Spark中有更高层次的方法来处理数据 —— 你会在下面的章节中看到 —— 但RDD是一个基本的数据结构。Spark RDD专业编程指南（ *https://oreil.ly/Lmf0r* ）将其描述为“在集群的各个节点上分割的元素集合，可以并行操作”。Spark的创建者建议将RDD视为一个大型的分布式电子表格。

从技术上讲，RDD是一种不可改变的、具有容错性的、并行的数据结构。Hien Luu（Apress）撰写的 *Beginning Apache Spark 2* 一书指出，RDD由五项信息代表：
- 一组分区，也就是构成数据集的块状物。
- 一组对父RDDs的依赖关系。
- 一个用于计算数据集中所有行的函数。
- 关于分区方案的元数据（可选）。
- 数据在集群中的位置（可选）。如果数据在HDFS上，那么它将是块位置的所在的位置。

当我们说RDD是一种并行数据结构时，这意味着一个大文件可以被分割到许多计算机上。在典型的使用案例中，一个数据集非常大，以至于无法容纳在一个节点上，所以它最终被分割到一个集群中的多个节点。关于分区，需要了解的一些基本情况是：
- 分区是Spark中并行的主要单位。
- Spark集群中的每个节点都包含一个或多个分区。
- 分区的数量是可以配置的。
- Spark提供了一个默认值，但你可以对它进行调整。
- 分区不跨越多个节点。
- Spark为集群的每个分区运行一个任务。

当你使用一个RDD时，数据中的每一行通常被表示为一个Java/Scala对象。这个对象的结构对Spark来说是未知的，所以除了提供 **filter** 和 **map** 这样的方法外，它不能帮助你进行处理，因为它知道如何处理从文件中创建的RDD（如20.2小节中讨论的），该文件被分割成大块并分布在许多计算机上。

当我说Spark不能帮助你处理时，这意味着Spark也提供了更高层次的技术，你会在后面的展示Spark **DataFrame** 和 **Dataset** 的示例中看到。当使用这些数据结构时，你将能够使用SQL查询，而且Spark有一个优化引擎，可以帮助你执行查询。

#### 创建RDD的三种方式

有三种方法来创建RDD：
- 在一个集合上调用 **parallelize**。
- 从一个或多个文件中读取数据到RDD中（正如你将在下一章看到的例子）。
- 在现有的RDD上调用转换方法来创建一个新的RDD。

解决方案中显示了 **parallelize** 方法。这种方法一般只用于测试，根据RDD Programming Guide ( *https://oreil.ly/X5BFx* )的“Parallelized Collections”，一节，它复制一个集合的内容，“以创建一个可以并行操作的分布式数据集”。

并行化方法需要一个可选的参数，以让你指定数据集可以被分成的分区数量：
```scala
    val rdd = spark.sparkContext.parallelize(nums, 20)
    rdd.getNumPartitions   // Int = 20
```

#### RDD方法

RDD上有几十种方法可用。你可以像往常一样，通过创建一个RDD，然后输入一个小数，在小数后面按Tab键，在REPL中看到这些方法。这包括常见的Scala集合方法的实现，如 **distinct**、**filter**、**foreach**、**map** 和 **take**，以及Spark特有的其他方法。有几十种方法，所以请看RDD类Scaladoc（ *https://oreil.ly/kl3rg* ）和 RDD Programming Guide（ *https://oreil.ly/7FwWA* ）以了解更多可用方法的细节。

### 另见