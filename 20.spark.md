# 20. Apache Spark

本章展示了Apache Spark（ *https://spark.apache.org* ）的一些例子，这是一个用于大规模数据处理的统一数据分析引擎。

Spark网站将其描述为一个“用于大规模数据处理的统一分析引擎”。这意味着它是一个大数据框架，可以让你用不同的技术分析你的数据 —— 比如把数据当作电子表格或数据库 —— 并在分布式集群上运行。你可以用Spark来分析那些大到跨越数千台计算机的数据集。
虽然Spark是为在计算机集群上处理巨大的数据集而设计的，但它的一个伟大之处在于，你只需用几个例子文件就可以学会如何在自己的计算机上使用Spark。

#### Spark 3.1.1 -- TODO 鸽子栏

&nbsp;&nbsp;&nbsp;&nbsp;本章的例子使用Spark 3.1.1，该版本于2021年3月发布，也是本文写作时的最新版本。Spark目前只能与Scala 2.12一起工作，所以本章的例子也使用Scala 2.12。然而，由于使用Spark的工作通常涉及到使用 **map** 和 **filter** 等集合方法，或者SQL查询，所以在这些例子中你几乎不会注意到Scala 2和Scala 3之间的区别。

本章的例子展示了如何在你自己的计算机上使用Spark，同时展示了在跨越数千台计算机的数据集上工作的关键概念。20.1小节展示了如何开始使用Spark，并深入研究其基本概念之一，即 *弹性分布式数据集（Resilient Distributed Dataset）*，或称RDD。RDD让你把数据当作一个大型的分布式电子表格。

第一个例子展示了如何在REPL内部，从一个集合中创建一个RDD，然后20.2小节展示了如何将一个数据文件读入RDD。它展示了几种读取数据文件的方法，使用Scala和Spark方法操作该数据，并将数据写入其他文件中。

接下来，使用MovieLens（ *https://movielens.org* ）的样本数据集，20.3小节展示了如何用Scala样例类对CSV文件中的一行进行建模，然后将该文件中的数据读入RDD并按需要进行操作。

然后，20.4小节展示了如何将RDD转换成 **DataFrame**。这个过程让你可以使用类似SQL的API来查询你的数据。这些例子展示了如何使用该查询语法，以及如何定义一个模式来表示你的数据。

从20.5小节开始，展示了如何将MovieLens CSV文件读入一个 **DataFrame**，并再次查询该数据。本示例将这一过程向前推进了一步，展示了如何在该数据上创建一个SQL视图，这让你可以使用常规的SQL查询字符串来查询该数据。

随着例子继续使用MovieLens数据集，20.6小节展示了如何将多个CSV文件读入 **DataFrames**，在这些文件上创建视图，然后用传统的SQL查询（包括Join）来查询数据，比如这样：
```scala
    val query = """
        select * from movies, ratings
        where movies.movieId == ratings.movieId
        and rating == 5.0
    """
```

最后，20.7小节展示了创建Spark应用程序的完整过程，该程序可以使用sbt打包成JAR文件并从命令行运行。

Spark是一个巨大的主题，关于它的书已经写了很多。本章内容的目的是让你迅速掌握一些主要的技术，并使之运行。有关Spark的更多细节，请看Bill Chambers和Matei Zaharia（O'Reilly）写的 *Spark: The Definitive Guide* 一书。

#### MovieLens数据集 -- TODO 鸽子栏

&nbsp;本章的例子使用MovieLens数据集（ *https://oreil.ly/3fSf0* ）。该数据集包括以CSV文件形式存在的真实世界的电影评分，该文件大小为数百兆字节，包含了超过62,000部电影的评分和细节。关于该数据集的细节，请参阅F.Maxwell Harper和Joseph A. Konstan的开创性论文，“The MovieLens Datasets: History and Context,” ACM Transactions on Interactive Intelligent Systems 5, no. 4 (2016): 1– 19, *https://doi.org/10.1145/2827872* .
