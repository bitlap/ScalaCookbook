# 20. Apache Spark

本章展示了Apache Spark（ *https://spark.apache.org* ）的一些例子，这是一个用于大规模数据处理的统一数据分析引擎。

Spark网站将其描述为一个“用于大规模数据处理的统一分析引擎”。这意味着它是一个大数据框架，可以让你用不同的技术分析你的数据 —— 比如把数据当作电子表格或数据库 —— 并在分布式集群上运行。你可以用Spark来分析那些大到跨越数千台计算机的数据集。
虽然Spark是为在计算机集群上处理巨大的数据集而设计的，但它的一个伟大之处在于，你只需用几个例子文件就可以学会如何在自己的计算机上使用Spark。

#### Spark 3.1.1 -- TODO 鸽子栏

&nbsp;&nbsp;&nbsp;&nbsp;本章的例子使用Spark 3.1.1，该版本于2021年3月发布，也是本文写作时的最新版本。Spark目前只能与Scala 2.12一起工作，所以本章的例子也使用Scala 2.12。然而，由于使用Spark的工作通常涉及到使用 **map** 和 **filter** 等集合方法，或者SQL查询，所以在这些例子中你几乎不会注意到Scala 2和Scala 3之间的区别。

本章的例子展示了如何在你自己的计算机上使用Spark，同时展示了在跨越数千台计算机的数据集上工作的关键概念。20.1小节展示了如何开始使用Spark，并深入研究其基本概念之一，即 *弹性分布式数据集（Resilient Distributed Dataset）*，或称RDD。RDD让你把数据当作一个大型的分布式电子表格。

第一个例子展示了如何在REPL内部，从一个集合中创建一个RDD，然后20.2小节展示了如何将一个数据文件读入RDD。它展示了几种读取数据文件的方法，使用Scala和Spark方法操作该数据，并将数据写入其他文件中。

接下来，使用MovieLens（ *https://movielens.org* ）的样本数据集，20.3小节展示了如何用Scala样例类对CSV文件中的一行进行建模，然后将该文件中的数据读入RDD并按需要进行操作。

然后，20.4小节展示了如何将RDD转换成 **DataFrame**。这个过程让你可以使用类似SQL的API来查询你的数据。这些例子展示了如何使用该查询语法，以及如何定义一个模式来表示你的数据。

从20.5小节开始，展示了如何将MovieLens CSV文件读入一个 **DataFrame**，并再次查询该数据。本示例将这一过程向前推进了一步，展示了如何在该数据上创建一个SQL视图，这让你可以使用常规的SQL查询字符串来查询该数据。

随着例子继续使用MovieLens数据集，20.6小节展示了如何将多个CSV文件读入 **DataFrames**，在这些文件上创建视图，然后用传统的SQL查询（包括Join）来查询数据，比如这样：
```scala
    val query = """
        select * from movies, ratings
        where movies.movieId == ratings.movieId
        and rating == 5.0
    """
```

最后，20.7小节展示了创建Spark应用程序的完整过程，该程序可以使用sbt打包成JAR文件并从命令行运行。

Spark是一个巨大的主题，关于它的书已经写了很多。本章内容的目的是让你迅速掌握一些主要的技术，并使之运行。有关Spark的更多细节，请看Bill Chambers和Matei Zaharia（O'Reilly）写的 *Spark: The Definitive Guide* 一书。

#### MovieLens数据集 -- TODO 鸽子栏

&nbsp;本章的例子使用MovieLens数据集（ *https://oreil.ly/3fSf0* ）。该数据集包括以CSV文件形式存在的真实世界的电影评分，该文件大小为数百兆字节，包含了超过62,000部电影的评分和细节。关于该数据集的细节，请参阅F.Maxwell Harper和Joseph A. Konstan的开创性论文，“The MovieLens Datasets: History and Context,” ACM Transactions on Interactive Intelligent Systems 5, no. 4 (2016): 1– 19, *https://doi.org/10.1145/2827872* .

## 20.1 开始使用Spark

### 问题

你以前从未使用过Spark，并想开始使用它。

### 解决方案

当你专业地使用Spark时，你的数据可能会分布在多台电脑上 —— 也许是成千上万台电脑 —— 但要开始使用Spark，你可以在一台电脑系统上以本地模式完成所有的工作。最简单的方法是启动Spark shell，创建一个数组，然后从那里开始。

Spark的安装说明可能会随着时间的推移而变化，但目前你只需从其下载页面（ *https://oreil.ly/O63Qu* ）下载Spark。下载的文件是一个 *tgz* 文件，所以只要解压并将其移动到你的 *bin* 目录，就像你手动安装其他下载的应用程序一样。例如，我把Spark安装在我的 */Users/al/bin* 目录下。

一旦你安装了Spark，就可以像这样启动Scala Spark shell：
```
    $ spark-shell
```

Spark shell是你通过 **scala** 命令得到的普通Scala shell的修改版，所以在Scala shell中能做的任何事情，在Spark shell中也能做，比如创建一个数组：
```scala
    val nums = Array.range(0, 100)    
```

一旦你有了像数组（array）或映射（map）这样的东西，就可以通过调用Spark Context的 **parallelize** 方法创建一个Spark Resilient Distributed Dataset（RDD）：
```scala
    scala> val rdd = spark.sparkContext.parallelize(nums)
    rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize
             at <console>:25
```

从输出中注意到，rdd的类型为 **RDD[Int]** 。正如你将在讨论中看到的，RDD是Spark的基本构建模块之一。现在你可以把它看作是一个像列表或数组一样的集合类，但它的数据可以分布在集群中的所有计算机上。它也有额外的方法可以被调用。下面是一些看起来很熟悉的Scala集合类的方法的例子：
```scala
    rdd.count
    rdd.first
    rdd.min
    rdd.max
    rdd.take(3)
    rdd.take(2).foreach(println)
    // Long = 100
    // Int = 0
    // Int = 0
    // Int = 99
    // Array[Int] = Array(0, 1, 2)
    // prints 0 and 1 on separate lines
```

这里有几个RDD方法，可能看起来不那么熟悉：
```scala
    // “sample” methods return random values from the RDD
    rdd.sample(false, 0.05).collect   // Array[Int] = Array(0, 16, 22, 27, 60, 73)
    rdd.takeSample(false, 5)          // Array[Int] = Array(35, 65, 31, 27, 1)
    
    rdd.mean                          // Double = 49.5
    rdd.stdev                         // Double = 28.866070047722115
    rdd.getNumPartitions              // Int = 8
    rdd.stats                         // StatCounter = (count: 100, mean: 49.500000,
                                      // stdev: 28.866070, max: 99.000000,
                                      // min: 0.000000)
```

你也可以使用熟悉的集合方法，如 **map** 和 **filter**：
```scala
    scala> rdd.map(_ + 5).filter(_ < 8)
    res0: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at filter at 
          <console>:26
    
    scala> rdd.filter(_ > 10).filter(_ < 20)
    res1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[12] at filter at
          <console>:26
```

然而，注意到这些方法并没有返回一个结果，至少不是你所期望的那个结果。在Spark中，像这样的转换方法是被惰性地评估的，所以我们把它们称为惰性或非严格的。为了从它们那里得到一个结果，你必须添加一个action方法。一个RDD有一个 **collect** 方法，这是一个action方法，可以强制运行所有之前的转换方法，然后把结果带回你的代码正在运行的计算机上。在这些例子中，添加 **collect** 会导致一个结果被计算出来：
```scala
    scala> rdd.map(_ + 5).filter(_ < 8).collect 
    res0: Array[Int] = Array(5, 6, 7)

    scala> rdd.filter(_ > 10).filter(_ < 20).collect
    res1: Array[Int] = Array(11, 12, 13, 14, 15, 16, 17, 18, 19)
```

### 讨论

在生产环境下，Spark会处理分布在计算机集群中的数据，但如图所示，在Spark的本地模式下，所有的处理都在你的本地计算机上完成。

#### spark对象和spark上下文

在解决方案中，我在Spark shell中用这两行代码创建了一个RDD：
```scala
    val nums = Array.range(0, 100)
    val rdd = spark.sparkContext.parallelize(nums)
```

正如你可能猜到的，spark是一个在shell中可用的预先构建的对象。你可以用shell的 **:type** 命令查看 **spark** 和 **spark.sparkContext** 的类型：
```scala
    scala> :type spark
    org.apache.spark.sql.SparkSession
    
    scala> :type spark.sparkContext
    org.apache.spark.SparkContext
```

在你的Spark编程中，你会经常使用这两个对象。因为你经常使用 **SparkContext**，所以在shell中有一个名为 **sc** 的快捷键可用，而不要再输入这个：
```scala
    val rdd = spark.sparkContext.parallelize(nums)
```

你可以直接打这个：
```scala
    val rdd = sc.parallelize(nums)
```

#### RDD

虽然在Spark中有更高层次的方法来处理数据 —— 你会在下面的章节中看到 —— 但RDD是一个基本的数据结构。Spark RDD专业编程指南（ *https://oreil.ly/Lmf0r* ）将其描述为“在集群的各个节点上分割的元素集合，可以并行操作”。Spark的创建者建议将RDD视为一个大型的分布式电子表格。

从技术上讲，RDD是一种不可改变的、具有容错性的、并行的数据结构。Hien Luu（Apress）撰写的 *Beginning Apache Spark 2* 一书指出，RDD由五项信息代表：
- 一组分区，也就是构成数据集的块状物。
- 一组对父RDDs的依赖关系。
- 一个用于计算数据集中所有行的函数。
- 关于分区方案的元数据（可选）。
- 数据在集群中的位置（可选）。如果数据在HDFS上，那么它将是块位置的所在的位置。

当我们说RDD是一种并行数据结构时，这意味着一个大文件可以被分割到许多计算机上。在典型的使用案例中，一个数据集非常大，以至于无法容纳在一个节点上，所以它最终被分割到一个集群中的多个节点。关于分区，需要了解的一些基本情况是：
- 分区是Spark中并行的主要单位。
- Spark集群中的每个节点都包含一个或多个分区。
- 分区的数量是可以配置的。
- Spark提供了一个默认值，但你可以对它进行调整。
- 分区不跨越多个节点。
- Spark为集群的每个分区运行一个任务。

当你使用一个RDD时，数据中的每一行通常被表示为一个Java/Scala对象。这个对象的结构对Spark来说是未知的，所以除了提供 **filter** 和 **map** 这样的方法外，它不能帮助你进行处理，因为它知道如何处理从文件中创建的RDD（如20.2小节中讨论的），该文件被分割成大块并分布在许多计算机上。

当我说Spark不能帮助你处理时，这意味着Spark也提供了更高层次的技术，你会在后面的展示Spark **DataFrame** 和 **Dataset** 的示例中看到。当使用这些数据结构时，你将能够使用SQL查询，而且Spark有一个优化引擎，可以帮助你执行查询。

#### 创建RDD的三种方式

有三种方法来创建RDD：
- 在一个集合上调用 **parallelize**。
- 从一个或多个文件中读取数据到RDD中（正如你将在下一章看到的例子）。
- 在现有的RDD上调用转换方法来创建一个新的RDD。

解决方案中显示了 **parallelize** 方法。这种方法一般只用于测试，根据RDD Programming Guide ( *https://oreil.ly/X5BFx* )的“Parallelized Collections”，一节，它复制一个集合的内容，“以创建一个可以并行操作的分布式数据集”。

并行化方法需要一个可选的参数，以让你指定数据集可以被分成的分区数量：
```scala
    val rdd = spark.sparkContext.parallelize(nums, 20)
    rdd.getNumPartitions   // Int = 20
```

#### RDD方法

RDD上有几十种方法可用。你可以像往常一样，通过创建一个RDD，然后输入一个小数，在小数后面按Tab键，在REPL中看到这些方法。这包括常见的Scala集合方法的实现，如 **distinct**、**filter**、**foreach**、**map** 和 **take**，以及Spark特有的其他方法。有几十种方法，所以请看RDD类Scaladoc（ *https://oreil.ly/kl3rg* ）和 RDD Programming Guide（ *https://oreil.ly/7FwWA* ）以了解更多可用方法的细节。

### 另见

- 关于Spark独立模式的页面 *spark.apache.org* （ *https://oreil.ly/AXtug* ）
- The Spark RDD Programming Guide（ *https://oreil.ly/7FwWA* ）

这些文章提供了关于Spark分区的更多细节：
- “How Data Partitioning in Spark Helps Achieve More Parallelism?”（ *https://oreil.ly/Qxter* ）
- “An Intro to Apache Spark Partitioning” （ *https://oreil.ly/67uLx* ）
- “Spark Partitions” （ *https://oreil.ly/d7ymD* ）


## 20.2 将文件读入Spark的RDD

### 问题

你想开始读取数据文件到Spark的RDD中。

### 解决方案

展示如何将数据文件读入RDD的典型例子是字数统计应用，因此，为了不让人失望，本示例展示了如何读取Abraham Lincoln的Gettysburg Address演讲文本，并找出文本中每个单词的使用次数。

启动Spark shell后，第一步是使用前一小节中介绍的 **SparkContext** 变量 **sc** 的 **textFile** 方法，来读取一个名为 *Gettysburg-Address.txt* 的文件：
```scala
    scala> val fileRdd = sc.textFile("Gettysburg-Address.txt") 
    fileRdd: org.apache.spark.rdd.RDD[String] = Gettysburg-Address.txt
            MapPartitionsRDD[1] at textFile at <console>:24
```

这个例子假设 *Gettysburg-Address.txt* 在当前目录下。

**textFile** 方法用于读取纯文本文件，它返回一个 **RDD[String]** 类型的对象。它也是惰性的，这意味着还没有发生什么。事实上，如果你拼错了文件名，你要等到一段时间后调用一个非惰性的action方法时才会发现。

剩下的字数计算的解决方案是纯Scala代码。你只需在RDD上调用一系列的转换方法，就能得到你想要的解决方案：
```scala
    val counts = fileRdd.map(_.replaceAll("[.,]", ""))
                       .map(_.replace("—", " "))
                       .flatMap(line => line.split(" "))
                       .map(word => (word, 1))
                       .reduceByKey(_ + _)
                       .sortBy(_._2)
                       .collect
```

在这个解决方案中，只有Spark独有的东西是：
- 在 **fileRdd** 上调用 **map** 方法（其类型为 **RDD[String]**）
- 在结尾处调用 **collect** 方法

正如前面提到的，因为所有的Spark转换方法都是惰性的，所以你必须调用一个急切的action方法，如 **collect**，以使action开始。

#### 粘贴多行表达式 -- TODO 鸽子栏

在Spark 3.1中，当你有一个像这样的多行表达式时，你必须用它的 **:paste** 命令将其粘贴到Spark shell中。其步骤是：
1. 在shell中输入 **:paste**
2. 从文本编辑器中复制你的表达式，并使用Command+V或类似的方法将其粘贴到REPL中
3. 输入Control+D来结束过去的操作

然后，shell会解释你的表达式。

### 讨论

下面解释一下这段代码是如何工作的。首先，我用这段代码将 **fileRdd** 创建为一个 **RDD[String]**：
```scala
    val fileRdd = sc.textFile("Gettysburg-Address.txt")
```

因为 **textFile** 是惰性的，所以实际上还没有发生什么。

接下来，因为我想计算字数，所以我用这两个 **map** 转换调用摆脱了文本中的小数、逗号和连字符：
```scala
    .map(_.replaceAll("[.,]", ""))
    .map(_.replace("—", " "))
```

然后，我使用这个 **flatMap** 表达式将文本转换为一个词的数组：
```scala
    .flatMap(line => line.split(" "))
```

如果你观察一下这两个 **map** 表达式和这个 **flatMap** 表达式的结果，你将会看到一个 **Array[String]**：
```scala
    Array(Four, score, and, seven, years, ago, our, fathers ...
```

为了从这个 **Array[String]** 中得到一个包含所有单词和它们出现次数的集合 —— **Map[String, Int]** —— 的解决方案，下一步是将每个单词变成一个元组：
```scala
    .map(word => (word, 1))
```

在这一点上，中间的数据看起来像这样：
```scala
    Array[(String, Int)] = Array((Four,1), (score,1), (and,1), (seven,1) ...
```

接下来，我使用了 **reduceByKey**：
```scala
    .reduceByKey(_ + _)
```

这就把中间数据转变成了这个数据结构：
```scala
    Array[(String, Int)] = Array((nobly,1), (under,1), (this,4), (have,5) ...
```

如上所示，这是一个元素是二元组的数组，每个元祖的第一个值是演讲中的一个词，第二个值是该词出现次数的计数。这就是 **reduceByKey** 为我们做的事情。

接下来，你可以通过第二个元组元素对该数据进行排序：
```scala
    .sortBy(_._2)
```

最后，你调用 **collect** 方法来强制运行所有的转换：
```scala
    .collect
```

因为这个数据结构的类型是 **Array[(String, Int)]**，你可以对它调用 **counts.foreach(println)**。其输出的结尾显示了演讲中最常见的词：
```scala
    scala> counts.foreach(println) [data omitted here]
    (here,8)
    (we,8)
    (to,8)
    (the,9)
    (that,13)
```

#### 读取文本文件到RDD的方法

有两种主要的方法可以将文本文件读入RDD：
- **sparkContext.textFile**
- **sparkContext.wholeTextFiles**

**textFile** 方法将一个文件作为一个行的集合来读取。它可以从本地文件系统中读取文件，也可以分别使用 “*hdfs://*” 和 “*s3a://*” URL从Hadoop或Amazon S3文件系统中读取。

根据 RDD Programming Guide（ **https://oreil.ly/7FwWA** ）所述，**textFile** “需要一个可选的第二个参数来控制文件的分区数量。默认情况下，Spark为文件的每个区块创建一个分区（HDFS中的区块默认为128MB），但你也可以通过传递一个更大的值来要求更多的分区数量。”

下面是一些使用 **textFile** 的例子：
```scala
    textFile("/foo/file.txt")                             // read a file, using the default
                                                          // number of partitions
    textFile("/foo/file.txt", 8)                          // same, but with 8 partitions
    textFile("/foo/bar.txt", "/foo/baz.txt")              // read multiple files
    textFile("/foo/ba*.txt")                              // read multiple files
    textFile("/foo/*")                                    // read all files in 'foo'
    textFile("/a/1.txt", "/b/2.txt")                      // multiple files in different
                                                          // directories
    textFile("hdfs://.../myfile.csv")                     // use a Hadoop URL
    textFile("s3a://.../myfile.csv")                      // use an Amazon S3 URL
```

请注意，s3a URL前缀是Hadoop S3连接器的名称，之前被命名为s3和s3n，所以你可能会在旧的文档中看到这些用法。

**wholeTextFiles** 方法将整个文件读成一个 **String**，并返回一个包含元组的RDD，所以其返回类型是 **RDD[(String, String)]**。元组中的第一个字符串是被读取的文件名，第二个字符串是文件的全部内容。这个例子展示了它的工作原理：
```scala
    scala> val rdd = sc.wholeTextFiles("Gettysburg-Address.txt")
    rdd: org.apache.spark.rdd.RDD[(String, String)] = Gettysburg-Address.txt
             MapPartitionsRDD[5] at wholeTextFiles at <console>:24
               
    scala> rdd.foreach(t => println(s"${t._1} | ${t._2.take(15)}")) 
    file:/Users/al/Projects/Books/ScalaCookbook2Examples/16_Ecosystem/Spark/ ↵
        SparkApp1/Gettysburg-Address.txt | Four score and
```

第二个表达式的输出显示该元组包含文件名和文件内容。

Spark还包含其他方法，用于将文件读入 **DataFrame** 或 **Dataset**：
- **spark.read.text()** 用于将文本文件读入 **DataFrame**。
- **spark.read.textFile()** 用于将文本文件读入 **Dataset[String]**。
- **spark.read.csv()** 和 **spark.read.format("csv").load("<path>")** 用于将CSV文件读入 **DataFrame**。

这些方法在下面的示例中都有展示。

#### 将RDD保存到磁盘上

#### 读取更为复杂的文本文件格式

### 另见


## 20.3 将CSV文件读入Spark RDD中

### 问题

### 解决方案

### 讨论

### 另见


## 20.4 将Spark像数据库一样用于DataFrame

### 问题

### 解决方案

### 讨论

### 另见


## 20.5 将数据文件读入Spark DataFrame

### 问题

### 解决方案

### 讨论

### 另见

## 20.6 对多个文件使用Spark SQL查询

### 问题

### 解决方案

### 讨论

### 另见


## 20.7 创建一个Spark批处理程序

### 问题

### 解决方案

### 讨论

### 另见
